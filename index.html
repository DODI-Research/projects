<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Proof-of-concept framework that uses AI services for automatically adding audio to silent videos.">
  <meta name="keywords" content="Auto-Foley, Auto-Foley Editor">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Auto-Foley</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=TODO:GOOGLE_ANALYTICS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'TODO:GOOGLE_ANALYTICS');
  </script>
  -->
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Automated Audio Source Analysis and Sound Effect Generation for Silent Videos using Vision Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://thomasmore.be/en/exp-dodi">Centre of Expertise for Sustainable Business and Digital Innovation - Thomas More</a>
			</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
				<a href="https://github.com/DODI-Research/auto-foley"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
				</a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/DODI-Research/auto-foley-editor"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-desktop"></i>
                  </span>
                  <span>Demo</span>
				</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
	<div class="columns is-centered has-text-centered">
      <div class="column is-three-quarters">
        <h3 class="has-text-centered" style="font-size: 1.2rem;">
          <strong>Auto-Foley</strong> is a proof-of-concept for a workflow that automates sound design for silent videos, using a vision language model for scene analysis and a text-to-sfx model for sound generation.
        </h3>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
		<div class="item item-train">
          <video poster="" id="train" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/train.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-reading">
          <video poster="" id="reading" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/reading.webm"
                    type="video/webm">
          </video>
        </div>
		<div class="item item-horses">
          <video poster="" id="horses" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/horses.webm"
                    type="video/webm">
          </video>
        </div>
		<div class="item item-bowling">
          <video poster="" id="bowling" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bowling.webm"
                    type="video/webm">
          </video>
        </div>
		<div class="item item-guitar">
          <video poster="" id="guitar" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/guitar.webm"
                    type="video/webm">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
			  The workflow instructs a vision language model to analyze the contents of the scenes and to generate text prompts for possible audio sources that could exist in them. 
			  These prompts are then sent to a text-to-sfx model.
			  The resulting audio files are then composited back onto the original video at their appropriate timings.
          </p>
        </div>
      </div>
    </div>
	<div class="columns is-centered">
      <div class="column">
		<center>
			<img src="static/images/workflow.jpg" width="900px">
		</center>
      </div>
    </div>
    <!--/ Overview. -->

	<!-- Proof-of-concept. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proof-of-concept</h2>
		<div class="content has-text-justified">
          <p>
		  This project explores the idea of using vision language models in sound design workflows.
          </p>
		  <p>
		  Rather than focusing on a specific implementation of that idea, the workflow follows a modular design where both the vision language model and the text-to-sfx services are replacable.
		  </p>
		  <p>
		  For the sake of simplicity and rapid development, the API services of OpenAI and ElevenLabs were used.
		  While functional, the quality of the output becomes increasingly worse the more complex or lengthy the videos are.
		  </p>
        </div>
      </div>
    </div>
    <!--/ Proof-of-concept. -->

    <!-- Comparison. -->
	<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison</h2>
        <div class="content has-text-justified">
          <p>
            Auto-foley takes a different approach compared to traditional video-to-audio architectures, like Google DeepMind's V2A technology.
          <p>
          </p>
			Converting scene analysis to text before generating audio from it causes synchronization details to get lost in translation. 
			While models like V2A generate synchronized audio directly from visual input.
          </p>
          <div class="columns is-centered comparison-table">
            <div class="column is-half">
              <p class="has-text-centered"><a href="https://deepmind.google/discover/blog/generating-audio-for-video/">V2A</a></p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/drums_v2a.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half">
              <p class="has-text-centered">Auto-foley</p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/drums_auto-foley.webm" type="video/webm">
              </video>
            </div>
          </div>
          <p class="mb-6">
            The Auto-Foley output has an incorrect rhythm pattern compared to the visual performance.
		  </p>
		  <p>
			The usage of a vision language model does offer a feature in its ability to describe ambient audio sources that should be heard, but are not seen in the visual input.
          </p>
		  <p>
			The limitations of the short and non-complex input, combined with the strength of ambient background noise being able to blend out small flaws in synchronization, makes Auto-Foley still suitable as the final step for AI video generation workflows. 
			A comparison with Tencent's Hunyuan Video shows how Auto-Foley performs in that scenario.
          </p>
		  <div class="columns is-multiline is-centered comparison-table">
            <div class="column is-half" style="padding-bottom: 0px;">
              <p class="has-text-centered"><a href="https://aivideo.hunyuan.tencent.com/">Hunyuan Video</a></p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/waterfall_hunyuan.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half" style="padding-bottom: 0px;">
              <p class="has-text-centered">Auto-foley</p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/waterfall_auto-foley.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half" style="padding-bottom: 0px;">
              <video controls muted playsinline height="100%">
                <source src="./static/videos/rally_hunyuan.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half" style="padding-bottom: 0px;">
              <video controls muted playsinline height="100%">
                <source src="./static/videos/rally_auto-foley.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half" style="padding-bottom: 0px;">
              <video controls muted playsinline height="100%">
                <source src="./static/videos/yoga_hunyuan.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half" style="padding-bottom: 0px;">
              <video controls muted playsinline height="100%">
                <source src="./static/videos/yoga_auto-foley.webm" type="video/webm">
              </video>
            </div>
          </div>
          <p>
            Auto-Foley shows how simply linking two readily available API services together can create results comparable to current solutions in certain environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Comparison. -->
	
	<!-- Editor. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Editor</h2>
		<div class="content has-text-justified">
          <p>
            Another benefit of the Auto-Foley workflow is that it enables human intervention between its two main components.
			Instead of producing a single audio track, Auto-Foley generates a separate file for each audio source. 
			This allows for manual synchronization adjustments, removal of unwanted sounds, or generation of new audio sources.
          </p>
          <p>			
			<a href="https://github.com/DODI-Research/auto-foley-editor">Auto-Foley Editor</a> demonstrates this editing capability in a Gradio interface.
          </p>
          <div class="columns is-centered comparison-table">
            <div class="column is-half">
              <p class="has-text-centered">Basic input</p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/editor_basic.webm" type="video/webm">
              </video>
            </div>
            <div class="column is-half">
              <p class="has-text-centered">Advanced control</p>
              <video controls muted playsinline height="100%">
                <source src="./static/videos/editor_advanced.webm" type="video/webm">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Editor. -->
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/DODI-Research/auto-foley" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Page template by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
